{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition(NER)\n",
    "\n",
    "単語をあらかじめ定められたタグに割り当てる\n",
    "\n",
    "- E.g.\n",
    "\n",
    "Sentence: “Taylor Swift will launch her new album in Apple Music.”\n",
    "\n",
    "- NER result:\n",
    "\n",
    "“Taylor[B-PER] Swift[I-PER] will[O] launch[O] her[O] new[O] album[O] in[O] Apple[B-ORG] Music[I-ORG].[O]”\n",
    "\n",
    "- PS:\n",
    "    - [O] means no meaning\n",
    "    - [B-PER]/[I-PER] means person name\n",
    "    - [B-ORG]/[I-ORG] means organization name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NERは主に２つのアプローチ方法が有る\n",
    "\n",
    "- Multi-class Classification-based\n",
    "- CRF based(Conditional Random Field)\n",
    "\n",
    "今回は多クラス分類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T13:13:24.633339Z",
     "start_time": "2020-07-05T13:13:24.561335Z"
    }
   },
   "source": [
    "## ライブラリーのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T13:16:08.995942Z",
     "start_time": "2020-07-05T13:16:07.686646Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from seqeval.metrics import f1_score\n",
    "from seqeval.metrics import classification_report,accuracy_score,f1_score\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T13:18:26.778082Z",
     "start_time": "2020-07-05T13:18:26.614597Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm,trange\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from transformers import BertForTokenClassification, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T13:18:28.752170Z",
     "start_time": "2020-07-05T13:18:27.569632Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras                             2.4.3\r\n",
      "Keras-Preprocessing               1.1.2\r\n",
      "torch                             1.5.1\r\n",
      "transformers                      3.0.1\r\n"
     ]
    }
   ],
   "source": [
    "# Check library version\n",
    "!pip list | grep -E 'transformers|torch|Keras'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T13:18:23.746120Z",
     "start_time": "2020-07-05T13:18:14.383854Z"
    }
   },
   "source": [
    "## データのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T13:24:10.545196Z",
     "start_time": "2020-07-05T13:24:07.658123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-07-05 13:24:08--  https://raw.githubusercontent.com/billpku/NLP_In_Action/master/data/ner_dataset.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.88.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.88.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 14159575 (14M) [text/plain]\n",
      "Saving to: ‘data/ner_dataset.csv’\n",
      "\n",
      "data/ner_dataset.cs 100%[===================>]  13.50M  13.4MB/s    in 1.0s    \n",
      "\n",
      "2020-07-05 13:24:10 (13.4 MB/s) - ‘data/ner_dataset.csv’ saved [14159575/14159575]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !wget https://raw.githubusercontent.com/billpku/NLP_In_Action/master/data/ner_dataset.csv -O data/ner_dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T13:24:26.371995Z",
     "start_time": "2020-07-05T13:24:26.367813Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = 'data/'\n",
    "data_file_address = \"data/ner_dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T13:25:21.144893Z",
     "start_time": "2020-07-05T13:25:20.309966Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>London</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>war</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demand</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>withdrawal</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>British</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>troops</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentence #           Word  POS    Tag\n",
       "0   Sentence: 1      Thousands  NNS      O\n",
       "1   Sentence: 1             of   IN      O\n",
       "2   Sentence: 1  demonstrators  NNS      O\n",
       "3   Sentence: 1           have  VBP      O\n",
       "4   Sentence: 1        marched  VBN      O\n",
       "5   Sentence: 1        through   IN      O\n",
       "6   Sentence: 1         London  NNP  B-geo\n",
       "7   Sentence: 1             to   TO      O\n",
       "8   Sentence: 1        protest   VB      O\n",
       "9   Sentence: 1            the   DT      O\n",
       "10  Sentence: 1            war   NN      O\n",
       "11  Sentence: 1             in   IN      O\n",
       "12  Sentence: 1           Iraq  NNP  B-geo\n",
       "13  Sentence: 1            and   CC      O\n",
       "14  Sentence: 1         demand   VB      O\n",
       "15  Sentence: 1            the   DT      O\n",
       "16  Sentence: 1     withdrawal   NN      O\n",
       "17  Sentence: 1             of   IN      O\n",
       "18  Sentence: 1        British   JJ  B-gpe\n",
       "19  Sentence: 1         troops  NNS      O"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fillna method can make same sentence with same sentence name\n",
    "df_data = pd.read_csv(data_file_address,sep=\",\",encoding=\"latin1\").fillna(method='ffill')\n",
    "df_data.head(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T13:25:52.404652Z",
     "start_time": "2020-07-05T13:25:52.380603Z"
    }
   },
   "source": [
    "POSの値一覧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T13:26:00.748635Z",
     "start_time": "2020-07-05T13:26:00.687127Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NNS', 'IN', 'VBP', 'VBN', 'NNP', 'TO', 'VB', 'DT', 'NN', 'CC',\n",
       "       'JJ', '.', 'VBD', 'WP', '``', 'CD', 'PRP', 'VBZ', 'POS', 'VBG',\n",
       "       'RB', ',', 'WRB', 'PRP$', 'MD', 'WDT', 'JJR', ':', 'JJS', 'WP$',\n",
       "       'RP', 'PDT', 'NNPS', 'EX', 'RBS', 'LRB', 'RRB', '$', 'RBR', ';',\n",
       "       'UH', 'FW'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.POS.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAGの値一覧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T13:26:24.454561Z",
     "start_time": "2020-07-05T13:26:24.399112Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['O', 'B-geo', 'B-gpe', 'B-per', 'I-geo', 'B-org', 'I-org', 'B-tim',\n",
       "       'B-art', 'I-art', 'I-per', 'I-gpe', 'I-tim', 'B-nat', 'B-eve',\n",
       "       'I-eve', 'I-nat'], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.Tag.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T13:26:58.251656Z",
     "start_time": "2020-07-05T13:26:58.063717Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O        887908\n",
       "B-geo     37644\n",
       "B-tim     20333\n",
       "B-org     20143\n",
       "I-per     17251\n",
       "B-per     16990\n",
       "I-org     16784\n",
       "B-gpe     15870\n",
       "I-geo      7414\n",
       "I-tim      6528\n",
       "B-art       402\n",
       "B-eve       308\n",
       "I-art       297\n",
       "I-eve       253\n",
       "B-nat       201\n",
       "I-gpe       198\n",
       "I-nat        51\n",
       "Name: Tag, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyse the Tag distribution\n",
    "df_data.Tag.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAGの意味一覧\n",
    "\n",
    "As show above, there are two parts for the tag name: \"position\"-\"meaning\"\n",
    "\n",
    "\n",
    "- B: begin, word at the first position\n",
    "- I: middle, word not at the first position,especially for phase\n",
    "- time: time, meaning time\n",
    "- per: person, meaning people name\n",
    "- geo: geography, meaning location name\n",
    "- O: mean other, set as a default tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データを解析してドキュメント形式に落とし込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T13:30:33.813565Z",
     "start_time": "2020-07-05T13:30:33.801893Z"
    }
   },
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"POS\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T13:32:55.141464Z",
     "start_time": "2020-07-05T13:32:49.122776Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get full document data struce\n",
    "getter = SentenceGetter(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T13:32:36.476956Z",
     "start_time": "2020-07-05T13:32:36.379507Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thousands',\n",
       " 'of',\n",
       " 'demonstrators',\n",
       " 'have',\n",
       " 'marched',\n",
       " 'through',\n",
       " 'London',\n",
       " 'to',\n",
       " 'protest',\n",
       " 'the',\n",
       " 'war',\n",
       " 'in',\n",
       " 'Iraq',\n",
       " 'and',\n",
       " 'demand',\n",
       " 'the',\n",
       " 'withdrawal',\n",
       " 'of',\n",
       " 'British',\n",
       " 'troops',\n",
       " 'from',\n",
       " 'that',\n",
       " 'country',\n",
       " '.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get sentence data\n",
    "sentences = [[s[0] for s in sent] for sent in getter.sentences]\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T13:32:44.636848Z",
     "start_time": "2020-07-05T13:32:44.299228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# Get tag labels data\n",
    "labels = [[s[2] for s in sent] for sent in getter.sentences]\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T13:34:51.139987Z",
     "start_time": "2020-07-05T13:34:51.095115Z"
    }
   },
   "outputs": [],
   "source": [
    "# TAGを名前からインデックスに変換する。\n",
    "\n",
    "tags_vals = list(set(df_data[\"Tag\"].values))\n",
    "\n",
    "\n",
    "# Add X  label for word piece support\n",
    "# Add [CLS] and [SEP] as BERT need\n",
    "tags_vals.append('X')\n",
    "tags_vals.append('[CLS]')\n",
    "tags_vals.append('[SEP]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T13:35:08.016974Z",
     "start_time": "2020-07-05T13:35:08.007436Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-art',\n",
       " 'B-eve',\n",
       " 'B-geo',\n",
       " 'B-gpe',\n",
       " 'B-nat',\n",
       " 'B-org',\n",
       " 'B-per',\n",
       " 'B-tim',\n",
       " 'I-art',\n",
       " 'I-eve',\n",
       " 'I-geo',\n",
       " 'I-gpe',\n",
       " 'I-nat',\n",
       " 'I-org',\n",
       " 'I-per',\n",
       " 'I-tim',\n",
       " 'O',\n",
       " 'X',\n",
       " '[CLS]',\n",
       " '[SEP]'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_vals = set(tags_vals)\n",
    "tags_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T13:35:33.851168Z",
     "start_time": "2020-07-05T13:35:33.843468Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set a dict for mapping id to tag name\n",
    "#tag2idx = {t: i for i, t in enumerate(tags_vals)}\n",
    "\n",
    "# Recommend to set it by manual define, good for reusing\n",
    "tag2idx={'B-art': 14,\n",
    " 'B-eve': 16,\n",
    " 'B-geo': 0,\n",
    " 'B-gpe': 13,\n",
    " 'B-nat': 12,\n",
    " 'B-org': 10,\n",
    " 'B-per': 4,\n",
    " 'B-tim': 2,\n",
    " 'I-art': 5,\n",
    " 'I-eve': 7,\n",
    " 'I-geo': 15,\n",
    " 'I-gpe': 8,\n",
    " 'I-nat': 11,\n",
    " 'I-org': 3,\n",
    " 'I-per': 6,\n",
    " 'I-tim': 1,\n",
    " 'X':17,\n",
    " 'O': 9,\n",
    " '[CLS]':18,\n",
    " '[SEP]':19}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T13:35:49.321462Z",
     "start_time": "2020-07-05T13:35:49.311227Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{14: 'B-art',\n",
       " 16: 'B-eve',\n",
       " 0: 'B-geo',\n",
       " 13: 'B-gpe',\n",
       " 12: 'B-nat',\n",
       " 10: 'B-org',\n",
       " 4: 'B-per',\n",
       " 2: 'B-tim',\n",
       " 5: 'I-art',\n",
       " 7: 'I-eve',\n",
       " 15: 'I-geo',\n",
       " 8: 'I-gpe',\n",
       " 11: 'I-nat',\n",
       " 3: 'I-org',\n",
       " 6: 'I-per',\n",
       " 1: 'I-tim',\n",
       " 17: 'X',\n",
       " 9: 'O',\n",
       " 18: '[CLS]',\n",
       " 19: '[SEP]'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mapping index to name\n",
    "tag2name={tag2idx[key] : key for key in tag2idx.keys()}\n",
    "tag2name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習データの作成\n",
    "\n",
    "Make raw data into trainable data for BERT, including:\n",
    "\n",
    "- Set gpu environment\n",
    "- Load tokenizer and tokenize\n",
    "- Set 3 embedding, token embedding, mask word embedding, segmentation embedding\n",
    "- Split data set into train and validate, then send them to dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T14:18:01.583406Z",
     "start_time": "2020-07-05T14:18:01.568212Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizerのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T14:22:39.376075Z",
     "start_time": "2020-07-05T14:22:37.320748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-07-05 14:22:37--  https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.104.5\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.104.5|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 213450 (208K) [text/plain]\n",
      "Saving to: ‘models/bert-base-cased/vocab.txt’\n",
      "\n",
      "models/bert-base-ca 100%[===================>] 208.45K   384KB/s    in 0.5s    \n",
      "\n",
      "2020-07-05 14:22:39 (384 KB/s) - ‘models/bert-base-cased/vocab.txt’ saved [213450/213450]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt -O \"models/bert-base-cased/vocab.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T14:23:32.368438Z",
     "start_time": "2020-07-05T14:23:32.325067Z"
    }
   },
   "outputs": [],
   "source": [
    "vocabulary = \"models/bert-base-cased/vocab.txt\"\n",
    "max_len = 45\n",
    "tokenizer=BertTokenizer(vocab_file=vocabulary,do_lower_case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer text\n",
    "\n",
    "- In hunggieface for bert, when come across OOV, will word piece the word\n",
    "- We need to adjust the labels base on the tokenize result, “##abc” need to set label \"X\"\n",
    "- Need to set \"[CLS]\" at front and \"[SEP]\" at the end, as what the paper do, BERT indexer should add [CLS] and [SEP] tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T14:24:08.769231Z",
     "start_time": "2020-07-05T14:23:33.094441Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.0,len:28\n",
      "texts:[CLS] Thousands of demons ##tra ##tors have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country . [SEP]\n",
      "No.0,len:28\n",
      "lables:[CLS] O O O X X O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O [SEP]\n",
      "No.1,len:29\n",
      "texts:[CLS] Iranian officials say they expect to get access to sealed sensitive parts of the plant Wednesday , after an I ##A ##EA surveillance system begins functioning . [SEP]\n",
      "No.1,len:29\n",
      "lables:[CLS] B-gpe O O O O O O O O O O O O O O B-tim O O O B-org X X O O O O O [SEP]\n",
      "No.2,len:44\n",
      "texts:[CLS] He ##lic ##op ##ter guns ##hips Saturday pounded militant hide ##outs in the Or ##ak ##zai tribal region , where many Taliban militants are believed to have fled to avoid an earlier military offensive in nearby South W ##azi ##rist ##an . [SEP]\n",
      "No.2,len:44\n",
      "lables:[CLS] O X X X O X B-tim O O O X O O B-geo X X O O O O O B-org O O O O O O O O O O O O O O B-geo I-geo X X X O [SEP]\n",
      "No.3,len:16\n",
      "texts:[CLS] They left after a tense hour - long stand ##off with riot police . [SEP]\n",
      "No.3,len:16\n",
      "lables:[CLS] O O O O O O X X O X O O O O [SEP]\n",
      "No.4,len:47\n",
      "texts:[CLS] U . N . relief coordinator Jan E ##gel ##and said Sunday , U . S . , Indonesian and Australian military helicopters are ferry ##ing out food and supplies to remote areas of western Ace ##h province that ground crews can not reach . [SEP]\n",
      "No.4,len:47\n",
      "lables:[CLS] B-geo X X X O O B-per I-per X X O B-tim O B-geo X X X O B-gpe O B-gpe O O O O X O O O O O O O O O B-geo X O O O O O O O O [SEP]\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts = []\n",
    "word_piece_labels = []\n",
    "i_inc = 0\n",
    "for word_list,label in (zip(sentences,labels)):\n",
    "    temp_lable = []\n",
    "    temp_token = []\n",
    "    \n",
    "    # Add [CLS] at the front \n",
    "    temp_lable.append('[CLS]')\n",
    "    temp_token.append('[CLS]')\n",
    "    \n",
    "    for word,lab in zip(word_list,label):\n",
    "        token_list = tokenizer.tokenize(word)\n",
    "        for m,token in enumerate(token_list):\n",
    "            temp_token.append(token)\n",
    "            if m==0:\n",
    "                temp_lable.append(lab)\n",
    "            else:\n",
    "                temp_lable.append('X')  \n",
    "                \n",
    "    # Add [SEP] at the end\n",
    "    temp_lable.append('[SEP]')\n",
    "    temp_token.append('[SEP]')\n",
    "    \n",
    "    tokenized_texts.append(temp_token)\n",
    "    word_piece_labels.append(temp_lable)\n",
    "    \n",
    "    if 5 > i_inc:\n",
    "        print(\"No.%d,len:%d\"%(i_inc,len(temp_token)))\n",
    "        print(\"texts:%s\"%(\" \".join(temp_token)))\n",
    "        print(\"No.%d,len:%d\"%(i_inc,len(temp_lable)))\n",
    "        print(\"lables:%s\"%(\" \".join(temp_lable)))\n",
    "    i_inc +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T14:15:50.215524Z",
     "start_time": "2020-07-05T14:15:47.790178Z"
    }
   },
   "source": [
    "### Set token embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T14:24:30.101405Z",
     "start_time": "2020-07-05T14:24:28.349512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  101 26159  1104  8568  4487  5067  1138  9639  1194  1498  1106  5641\n",
      "  1103  1594  1107  5008  1105  4555  1103 10602  1104  1418  2830  1121\n",
      "  1115  1583   119   102     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "# Make text token into id\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=max_len, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "print(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T14:25:09.463538Z",
     "start_time": "2020-07-05T14:25:08.889791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18  9  9  9 17 17  9  9  9  0  9  9  9  9  9  0  9  9  9  9  9 13  9  9\n",
      "  9  9  9 19  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9]\n"
     ]
    }
   ],
   "source": [
    "# Make label into id, pad with \"O\" meaning others\n",
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in word_piece_labels],\n",
    "                     maxlen=max_len, value=tag2idx[\"O\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "print(tags[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set mask word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T14:28:12.744900Z",
     "start_time": "2020-07-05T14:28:10.838963Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For fine tune of predict, with token mask is 1,pad token is 0\n",
    "attention_masks = [[int(i>0) for i in ii] for ii in input_ids]\n",
    "attention_masks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set segment embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T14:29:08.375352Z",
     "start_time": "2020-07-05T14:29:08.315924Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since only one sentence, all the segment set to 0\n",
    "segment_ids = [[0] * len(input_id) for input_id in input_ids]\n",
    "segment_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T14:31:25.356893Z",
     "start_time": "2020-07-05T14:31:25.284158Z"
    }
   },
   "outputs": [],
   "source": [
    "tr_inputs, val_inputs, tr_tags, val_tags,tr_masks, val_masks,tr_segs, val_segs = train_test_split(input_ids, tags,attention_masks,segment_ids, \n",
    "                                                            random_state=4, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T14:31:55.089764Z",
     "start_time": "2020-07-05T14:31:55.078756Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33571, 14388, 33571, 14388)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tr_inputs),len(val_inputs),len(tr_segs),len(val_segs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T14:31:39.461022Z",
     "start_time": "2020-07-05T14:31:39.000187Z"
    }
   },
   "outputs": [],
   "source": [
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "tr_segs = torch.tensor(tr_segs)\n",
    "val_segs = torch.tensor(val_segs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T14:32:08.749697Z",
     "start_time": "2020-07-05T14:32:08.743898Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set batch num\n",
    "batch_num = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T14:32:21.079828Z",
     "start_time": "2020-07-05T14:32:21.071751Z"
    }
   },
   "outputs": [],
   "source": [
    "# Only set token embedding, attention embedding, no segment embedding\n",
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "# Drop last can make batch training better for the last one\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_num,drop_last=True)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T14:35:54.897008Z",
     "start_time": "2020-07-05T14:34:09.792732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-07-05 14:34:10--  https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.101.22\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.101.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 435779157 (416M) [application/octet-stream]\n",
      "Saving to: ‘models/bert-base-cased/pytorch_modelrch_model.bin’\n",
      "\n",
      "models/bert-base-ca 100%[===================>] 415.59M  4.49MB/s    in 1m 43s  \n",
      "\n",
      "2020-07-05 14:35:54 (4.02 MB/s) - ‘models/bert-base-cased/pytorch_modelrch_model.bin’ saved [435779157/435779157]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin -O models/bert-base-cased/pytorch_modelrch_model.bin\n",
    "# !wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json -O models/bert-base-cased/config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T14:36:49.240405Z",
     "start_time": "2020-07-05T14:36:45.566882Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at models/bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at models/bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_file_address = 'models/bert-base-cased'\n",
    "# Will load config and weight with from_pretrained()\n",
    "model = BertForTokenClassification.from_pretrained(model_file_address,num_labels=len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T14:37:50.541086Z",
     "start_time": "2020-07-05T14:37:50.537398Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.cuda()\n",
    "# Add multi GPU support\n",
    "# if n_gpu >1:\n",
    "#     model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T14:37:52.198578Z",
     "start_time": "2020-07-05T14:37:52.194744Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set epoch and grad max num\n",
    "epochs = 5\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# Cacluate train optimiazaion num\n",
    "num_train_optimization_steps = int( math.ceil(len(tr_inputs) / batch_num) / 1) * epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T14:38:14.397794Z",
     "start_time": "2020-07-05T14:38:14.393672Z"
    }
   },
   "source": [
    "### Set fine tuning method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T14:38:27.897747Z",
     "start_time": "2020-07-05T14:38:27.893067Z"
    }
   },
   "outputs": [],
   "source": [
    "# True: fine tuning all the layers \n",
    "# False: only fine tuning the classifier layers\n",
    "FULL_FINETUNING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T14:38:37.464258Z",
     "start_time": "2020-07-05T14:38:37.453510Z"
    }
   },
   "outputs": [],
   "source": [
    "if FULL_FINETUNING:\n",
    "    # Fine tune model all layer parameters\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    # Only fine tune classifier parameters\n",
    "    param_optimizer = list(model.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-05T14:39:14.304Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 33571\n",
      "  Batch size = 32\n",
      "  Num steps = 5250\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "print(\"***** Running training *****\")\n",
    "print(\"  Num examples = %d\"%(len(tr_inputs)))\n",
    "print(\"  Batch size = %d\"%(batch_num))\n",
    "print(\"  Num steps = %d\"%(num_train_optimization_steps))\n",
    "for _ in trange(epochs,desc=\"Epoch\"):\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(b_input_ids, token_type_ids=None,\n",
    "        attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss, scores = outputs[:2]\n",
    "        if n_gpu>1:\n",
    "            # When multi gpu, average it\n",
    "            loss = loss.mean()\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    # print train loss per epoch\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T14:39:56.522509Z",
     "start_time": "2020-07-05T14:39:56.518147Z"
    }
   },
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T14:40:24.629060Z",
     "start_time": "2020-07-05T14:40:24.552884Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4322270f02fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Make dir if not exits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_out_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_out_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "bert_out_address = 'models/bert_out_model/en09'\n",
    "\n",
    "# Make dir if not exits\n",
    "if not os.path.exists(bert_out_address):\n",
    "        os.makedirs(bert_out_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T14:40:40.862571Z",
     "start_time": "2020-07-05T14:40:40.850405Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c153568a8251>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save a trained model, configuration and tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_to_save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'module'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmodel\u001b[0m  \u001b[0;31m# Only save the model it-self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Save a trained model, configuration and tokenizer\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T14:40:52.687285Z",
     "start_time": "2020-07-05T14:40:52.672368Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-75ca6c3a94f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# If we save using the predefined names, we can load using `from_pretrained`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutput_model_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_out_address\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pytorch_model.bin\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0moutput_config_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_out_address\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# If we save using the predefined names, we can load using `from_pretrained`\n",
    "output_model_file = os.path.join(bert_out_address, \"pytorch_model.bin\")\n",
    "output_config_file = os.path.join(bert_out_address, \"config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T14:41:08.723339Z",
     "start_time": "2020-07-05T14:41:08.708530Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ef4e0294bb8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save model into file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_to_save\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_model_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel_to_save\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_config_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_out_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Save model into file\n",
    "torch.save(model_to_save.state_dict(), output_model_file)\n",
    "model_to_save.config.to_json_file(output_config_file)\n",
    "tokenizer.save_vocabulary(bert_out_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T14:41:44.531195Z",
     "start_time": "2020-07-05T14:41:44.524553Z"
    }
   },
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T14:41:55.821531Z",
     "start_time": "2020-07-05T14:41:55.809653Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BertForTokenClassification' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-82cf41acd498>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForTokenClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_out_address\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag2idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'BertForTokenClassification' is not defined"
     ]
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(bert_out_address,num_labels=len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T14:42:18.639830Z",
     "start_time": "2020-07-05T14:42:18.626122Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-83171c7f96c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Set model to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Set model to GPU\n",
    "model.cuda()\n",
    "\n",
    "if n_gpu >1:\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T14:42:43.930382Z",
     "start_time": "2020-07-05T14:42:43.882529Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-76a2653d4c2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0meval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnb_eval_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_eval_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "print(\"***** Running evaluation *****\")\n",
    "print(\"  Num examples ={}\".format(len(val_inputs)))\n",
    "print(\"  Batch size = {}\".format(batch_num))\n",
    "for step, batch in enumerate(valid_dataloader):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    input_ids, input_mask, label_ids = batch\n",
    "    \n",
    "#     if step > 2:\n",
    "#         break\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, token_type_ids=None,\n",
    "        attention_mask=input_mask,)\n",
    "        # For eval mode, the first result of outputs is logits\n",
    "        logits = outputs[0] \n",
    "    \n",
    "    # Get NER predict result\n",
    "    logits = torch.argmax(F.log_softmax(logits,dim=2),dim=2)\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    \n",
    "    \n",
    "    # Get NER true result\n",
    "    label_ids = label_ids.to('cpu').numpy()\n",
    "    \n",
    "    \n",
    "    # Only predict the real word, mark=0, will not calculate\n",
    "    input_mask = input_mask.to('cpu').numpy()\n",
    "    \n",
    "    # Compare the valuable predict result\n",
    "    for i,mask in enumerate(input_mask):\n",
    "        # Real one\n",
    "        temp_1 = []\n",
    "        # Predict one\n",
    "        temp_2 = []\n",
    "        \n",
    "        for j, m in enumerate(mask):\n",
    "            # Mark=0, meaning its a pad word, dont compare\n",
    "            if m:\n",
    "                if tag2name[label_ids[i][j]] != \"X\" and tag2name[label_ids[i][j]] != \"[CLS]\" and tag2name[label_ids[i][j]] != \"[SEP]\" : # Exclude the X label\n",
    "                    temp_1.append(tag2name[label_ids[i][j]])\n",
    "                    temp_2.append(tag2name[logits[i][j]])\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "            \n",
    "        y_true.append(temp_1)\n",
    "        y_pred.append(temp_2)\n",
    "\n",
    "        \n",
    "\n",
    "print(\"f1 socre: %f\"%(f1_score(y_true, y_pred)))\n",
    "print(\"Accuracy score: %f\"%(accuracy_score(y_true, y_pred)))\n",
    "\n",
    "# Get acc , recall, F1 result report\n",
    "report = classification_report(y_true, y_pred,digits=4)\n",
    "\n",
    "# Save the report into file\n",
    "output_eval_file = os.path.join(bert_out_address, \"eval_results.txt\")\n",
    "with open(output_eval_file, \"w\") as writer:\n",
    "    print(\"***** Eval results *****\")\n",
    "    print(\"\\n%s\"%(report))\n",
    "    print(\"f1 socre: %f\"%(f1_score(y_true, y_pred)))\n",
    "    print(\"Accuracy score: %f\"%(accuracy_score(y_true, y_pred)))\n",
    "    \n",
    "    writer.write(\"f1 socre:\\n\")\n",
    "    writer.write(str(f1_score(y_true, y_pred)))\n",
    "    writer.write(\"\\n\\nAccuracy score:\\n\")\n",
    "    writer.write(str(accuracy_score(y_true, y_pred)))\n",
    "    writer.write(\"\\n\\n\")  \n",
    "    writer.write(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
